{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKU-110K Dataset Processing Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Verify Dataset Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls /content/drive/MyDrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Extract Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "src_file = '/content/drive/MyDrive/SKU110K_fixed.tar.gz'\n",
    "dest_path = '/content/drive/MyDrive/datasets/SKU110K_data'\n",
    "\n",
    "with tarfile.open(src_file, \"r:gz\") as tar:\n",
    "    tar.extractall(path=dest_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Check Directory Structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_path = \"/content/drive/MyDrive/datasets/SKU110K_data/SKU110K_fixed\"\n",
    "\n",
    "for root, dirs, files in os.walk(base_path):\n",
    "    print(f\"ðŸ“‚ Directory: {root}\")\n",
    "    print(f\"ðŸ“¦ Subdirectories: {dirs}\")\n",
    "    print(f\"ðŸ“„ Files: {files}\")\n",
    "    break  # Show first level only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Validation & Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Check for Corrupted Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image_dir = os.path.join(base_path, 'images')\n",
    "bad_images = []\n",
    "\n",
    "for img_name in os.listdir(image_dir)[:500]:  # Sample check\n",
    "    img_path = os.path.join(image_dir, img_name)\n",
    "    try:\n",
    "        img = Image.open(img_path)\n",
    "        img.verify()\n",
    "    except Exception as e:\n",
    "        bad_images.append((img_name, str(e)))\n",
    "\n",
    "print(f\"âŒ Found {len(bad_images)} corrupted images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Load & Validate Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ann_path = os.path.join(base_path, \"annotations/annotations_test.csv\")\n",
    "\n",
    "# Load with correct column names\n",
    "df = pd.read_csv(ann_path, names=[\n",
    "    \"image_name\", \"x1\", \"y1\", \"x2\", \"y2\", \"class\", \"image_width\", \"image_height\"\n",
    "])\n",
    "\n",
    "# Check structure\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Convert to numeric\n",
    "numeric_cols = [\"x1\", \"y1\", \"x2\", \"y2\", \"image_width\", \"image_height\"]\n",
    "df[numeric_cols] = df[numeric_cols].apply(pd.to_numeric)\n",
    "\n",
    "# Check for invalid bounding boxes\n",
    "invalid_coords = df[(df.x1 >= df.x2) | (df.y1 >= df.y2)]\n",
    "print(f\"âŒ Found {len(invalid_coords)} invalid boxes.\")\n",
    "\n",
    "# Check for out-of-bounds coordinates\n",
    "out_of_bounds = df[\n",
    "    (df.x2 > df.image_width) | (df.y2 > df.image_height) |\n",
    "    (df.x1 < 0) | (df.y1 < 0)\n",
    "]\n",
    "print(f\"âŒ Found {len(out_of_bounds)} out-of-bounds boxes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. COCO Format Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Convert Annotations to COCO Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "def convert_to_coco(df):\n",
    "    images = []\n",
    "    annotations = []\n",
    "    categories = [{\"id\": 1, \"name\": \"object\"}]\n",
    "\n",
    "    image_id_map = {}  # Maps image_name â†’ image_id\n",
    "    annotation_id = 1\n",
    "\n",
    "    for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "        img_name = row[\"image_name\"]\n",
    "        if img_name not in image_id_map:\n",
    "            image_id = len(image_id_map) + 1\n",
    "            image_id_map[img_name] = image_id\n",
    "            images.append({\n",
    "                \"id\": image_id,\n",
    "                \"file_name\": img_name,\n",
    "                \"width\": row[\"image_width\"],\n",
    "                \"height\": row[\"image_height\"]\n",
    "            })\n",
    "\n",
    "        x, y = row[\"x1\"], row[\"y1\"]\n",
    "        width, height = row[\"x2\"] - x, row[\"y2\"] - y\n",
    "\n",
    "        annotations.append({\n",
    "            \"id\": annotation_id,\n",
    "            \"image_id\": image_id_map[img_name],\n",
    "            \"category_id\": 1,\n",
    "            \"bbox\": [x, y, width, height],\n",
    "            \"area\": width * height,\n",
    "            \"iscrowd\": 0\n",
    "        })\n",
    "        annotation_id += 1\n",
    "\n",
    "    return {\"images\": images, \"annotations\": annotations, \"categories\": categories}\n",
    "\n",
    "# Convert and save\n",
    "coco = convert_to_coco(df)\n",
    "output_json = \"/content/drive/MyDrive/datasets/SKU110K_project/processed/annotations_coco.json\"\n",
    "os.makedirs(os.path.dirname(output_json), exist_ok=True)\n",
    "\n",
    "with open(output_json, \"w\") as f:\n",
    "    json.dump(coco, f)\n",
    "\n",
    "print(f\"âœ… COCO annotations saved to: {output_json}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metadata Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Create Dataset Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dataset_profile = {\n",
    "    \"dataset_name\": \"SKU-110K\",\n",
    "    \"version\": \"v1.0\",\n",
    "    \"num_images\": len(coco['images']),\n",
    "    \"num_annotations\": len(coco['annotations']),\n",
    "    \"class_names\": [cat['name'] for cat in coco['categories']],\n",
    "    \"raw_images_path\": \"/datasets/SKU110K_project/raw/images/\",\n",
    "    \"annotations_path\": output_json,\n",
    "    \"created_at\": datetime.now().isoformat()\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_output_path = \"/content/drive/MyDrive/datasets/SKU110K_project/metadata/dataset_profile.json\"\n",
    "os.makedirs(os.path.dirname(metadata_output_path), exist_ok=True)\n",
    "\n",
    "with open(metadata_output_path, \"w\") as f:\n",
    "    json.dump(dataset_profile, f, indent=4)\n",
    "\n",
    "print(\"ðŸ“Š Dataset metadata saved successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Planogram Data Processing (Mock Example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Generate Mock Planogram Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_planogram = [\n",
    "    {\"store_id\": \"store_01\", \"product_id\": \"SKU_001\", \"x\": 100, \"y\": 200, \"width\": 50, \"height\": 120, \"shelf\": \"top\"},\n",
    "    {\"store_id\": \"store_01\", \"product_id\": \"SKU_002\", \"x\": 180, \"y\": 210, \"width\": 60, \"height\": 110, \"shelf\": \"middle\"},\n",
    "    {\"store_id\": \"store_01\", \"product_id\": \"SKU_003\", \"x\": 260, \"y\": 220, \"width\": 55, \"height\": 115, \"shelf\": \"bottom\"}\n",
    "]\n",
    "\n",
    "# Save as JSON\n",
    "planogram_path = \"/content/drive/MyDrive/datasets/SKU110K_project/raw/planogram.json\"\n",
    "os.makedirs(os.path.dirname(planogram_path), exist_ok=True)\n",
    "\n",
    "with open(planogram_path, \"w\") as f:\n",
    "    json.dump(mock_planogram, f, indent=4)\n",
    "\n",
    "print(f\"âœ… Mock planogram saved to: {planogram_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Convert to CSV & SQLite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and validate\n",
    "with open(planogram_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "df_planogram = pd.DataFrame(data)\n",
    "\n",
    "# Save as CSV\n",
    "csv_output_path = \"/content/drive/MyDrive/datasets/SKU110K_project/processed/planogram_clean.csv\"\n",
    "df_planogram.to_csv(csv_output_path, index=False)\n",
    "\n",
    "# Save to SQLite\n",
    "import sqlite3\n",
    "db_path = \"/content/drive/MyDrive/datasets/SKU110K_project/processed/planogram.db\"\n",
    "conn = sqlite3.connect(db_path)\n",
    "df_planogram.to_sql(\"planogram\", conn, if_exists=\"replace\", index=False)\n",
    "conn.close()\n",
    "\n",
    "print(\"âœ… Planogram processing complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}